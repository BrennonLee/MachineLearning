{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5: Naive Bayes, Cross-Validation, and VC Dimension \n",
    "***\n",
    "\n",
    "**Name**: Brennon Lee\n",
    "\n",
    "***\n",
    "\n",
    "This assignment is due on Moodle by **5pm on Wednesday April 18th**. Your solutions to theoretical questions should be done in Markdown/MathJax directly below the associated question.  Your solutions to computational questions should include any specified Python code and results as well as written commentary on your conclusions.  Remember that you are encouraged to discuss the problems with your instructors and classmates, but **you must write all code and solutions on your own**.  For a refresher on the course **Collaboration Policy** click [here](https://github.com/chrisketelsen/CSCI-4622-Machine-Learning/blob/master/resources/syllabus.md#collaboration-policy).\n",
    "\n",
    "**NOTES**: \n",
    "\n",
    "- Do **NOT** load or use any Python packages that are not available in Anaconda 3.6. \n",
    "- Some problems with code may be autograded.  If we provide a function API **do not** change it.  If we do not provide a function API then you're free to structure your code however you like. \n",
    "- Submit only this Jupyter notebook to Moodle.  Do not compress it using tar, rar, zip, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T14:42:01.964923Z",
     "start_time": "2018-04-03T14:42:01.249718Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pickle, gzip\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [15 points] Problem 1 - Naive Bayes for Tennis Prediction \n",
    "***\n",
    "\n",
    "Suppose you are trying to learn a person's decision whether to play tennis or not on a given day using features corresponding to precipitation forecast, temperature, humidity, and wind. You're given the following training data: \n",
    "\n",
    "$$\n",
    "\\begin{array}{c|c|c|c|c}\n",
    "\\textbf{Forecast} & \\textbf{Temp} & \\textbf{Humidity} & \\textbf{Wind} & \\textbf{PlayTennis} \\\\\n",
    "\\hline \n",
    "\\textrm{sunny} & \\textrm{hot} & \\textrm{high} & \\textrm{weak} & \\textbf{No} \\\\ \n",
    "\\textrm{sunny} & \\textrm{hot} & \\textrm{high} & \\textrm{strong} & \\textbf{No} \\\\ \n",
    "\\textrm{overcast} & \\textrm{hot} & \\textrm{high} & \\textrm{weak} & \\textbf{Yes} \\\\ \n",
    "\\textrm{rainy} & \\textrm{mild} & \\textrm{high} & \\textrm{weak} & \\textbf{Yes} \\\\ \n",
    "\\textrm{rainy} & \\textrm{cool} & \\textrm{normal} & \\textrm{weak} & \\textbf{Yes} \\\\ \n",
    "\\textrm{rainy} & \\textrm{cool} & \\textrm{normal} & \\textrm{strong} & \\textbf{No} \\\\ \n",
    "\\textrm{overcast} & \\textrm{cool} & \\textrm{normal} & \\textrm{strong} & \\textbf{Yes} \\\\ \n",
    "\\textrm{sunny} & \\textrm{mild} & \\textrm{high} & \\textrm{weak} & \\textbf{No} \\\\ \n",
    "\\textrm{sunny} & \\textrm{cool} & \\textrm{normal} & \\textrm{weak} & \\textbf{Yes} \\\\ \n",
    "\\textrm{rainy} & \\textrm{mild} & \\textrm{normal} & \\textrm{weak} & \\textbf{Yes} \\\\ \n",
    "\\textrm{sunny} & \\textrm{mild} & \\textrm{normal} & \\textrm{strong} & \\textbf{Yes} \\\\ \n",
    "\\textrm{overcast} & \\textrm{mild} & \\textrm{high} & \\textrm{strong} & \\textbf{Yes} \\\\ \n",
    "\\textrm{overcast} & \\textrm{hot} & \\textrm{normal} & \\textrm{weak} & \\textbf{Yes} \\\\ \n",
    "\\textrm{rainy} & \\textrm{mild} & \\textrm{high} & \\textrm{strong} & \\textbf{No} \\\\ \n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "**Part A**: Estimate the priors $p(\\textrm{PlayTennis=Yes})$ and $p(\\textrm{PlayTennis=No})$ from the training data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "source": [
    "$$p(PlayTennis = Yes) = \\frac{\\text{# of Yes Terms in PlayTennis Column}}{\\text{# Terms in PlayTennis Column}} = \\boxed{\\frac{9}{14}}$$\n",
    "\n",
    "$$p(PlayTennis = No) = \\frac{\\text{# of No Terms in PlayTennis Column}}{\\text{# Terms in PlayTennis Column}} = \\boxed{\\frac{5}{14}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: For each feature\n",
    "- state the vocabulary $V$ for the feature \n",
    "- estimate the class-conditional probabilities $p(\\textrm{feature value} \\mid \\textrm{PlayTennis})$ from the training data using Laplace add-one smoothing. Show your work. \n",
    "\n",
    "**Note**: There is no need to include any `UNK` features for this data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### The general formual with Laplace add-one smoothing factor is $P(\\text{feature | PlayTennis}) = \\frac{\\text{# of instance of term in Class + 1}}{\\text{# of total words in Class + |V|}}$\n",
    "\n",
    "###### For feature Forecast, the vocab is = { Sunny, Overcast, Rainy }\n",
    "$$P(\\text{Forecast = Sunny} \\ | \\ \\text{PlayTennis}) = \\frac{2 + 1}{9 + 3} = \\boxed{\\frac{1}{4}}$$\n",
    "\n",
    "$$P(\\text{Forecast = Overcast} \\ | \\ \\text{PlayTennis}) = \\frac{4 + 1}{9 + 3} = \\boxed{\\frac{5}{12}}$$\n",
    "\n",
    "$$P(\\text{Forecast = Rainy} \\ | \\ \\text{PlayTennis}) = \\frac{3 + 1}{9 + 3} = \\boxed{\\frac{1}{3}}$$\n",
    "\n",
    "$$P(\\text{Forecast = Sunny} \\ | \\ \\text{NOT PlayTennis}) = \\boxed{\\frac{1}{2}}$$\n",
    "\n",
    "$$P(\\text{Forecast = Overcast} \\ | \\ \\text{NOT PlayTennis}) = \\boxed{\\frac{1}{8}}$$\n",
    "\n",
    "$$P(\\text{Forecast = Rainy} \\ | \\ \\text{NOT PlayTennis}) = \\boxed{\\frac{3}{8}}$$\n",
    "\n",
    "###### For feature Temp, the vocab is = { Hot, Mild, Cool }\n",
    "$$P(\\text{Temp = Hot} \\ | \\ \\text{PlayTennis}) = \\frac{2 + 1}{9 + 3} = \\boxed{\\frac{1}{4}}$$\n",
    "\n",
    "$$P(\\text{Temp = Mild} \\ | \\ \\text{PlayTennis}) = \\frac{4 + 1}{9 + 3} = \\boxed{\\frac{5}{12}}$$\n",
    "\n",
    "$$P(\\text{Temp = Cool} \\ | \\ \\text{PlayTennis}) = \\frac{3 + 1}{9 + 3} = \\boxed{\\frac{1}{3}}$$\n",
    "\n",
    "$$P(\\text{Temp = Hot} \\ | \\ \\text{NOT PlayTennis}) = \\boxed{\\frac{3}{8}}$$\n",
    "\n",
    "$$P(\\text{Temp = Mild} \\ | \\ \\text{NOT PlayTennis}) = \\boxed{\\frac{3}{8}}$$\n",
    "\n",
    "$$P(\\text{Temp = Cool} \\ | \\ \\text{NOT PlayTennis}) = \\boxed{\\frac{1}{4}}$$\n",
    "\n",
    "###### For feature Humidity, the vocab is = { High, Normal }\n",
    "$$P(\\text{Humidity = High} \\ | \\ \\text{PlayTennis}) = \\frac{3 + 1}{9 + 2} = \\boxed{\\frac{4}{11}}$$\n",
    "\n",
    "$$P(\\text{Humidity = Normal} \\ | \\ \\text{PlayTennis}) = \\frac{6 + 1}{9 + 2} = \\boxed{\\frac{7}{11}}$$\n",
    "\n",
    "$$P(\\text{Humidity = High} \\ | \\ \\text{NOT PlayTennis}) = \\boxed{\\frac{5}{7}}$$\n",
    "\n",
    "$$P(\\text{Humidity = Normal} \\ | \\ \\text{NOT PlayTennis}) = \\boxed{\\frac{2}{7}}$$\n",
    "\n",
    "###### For feature Wind, the vocab is = { Weak, Strong }\n",
    "$$P(\\text{Windy = Strong} \\ | \\ \\text{PlayTennis}) = \\frac{3 + 1}{9 + 2} = \\boxed{\\frac{4}{11}}$$\n",
    "\n",
    "$$P(\\text{Windy = Weak} \\ | \\ \\text{PlayTennis}) = \\frac{6 + 1}{9 + 2} = \\boxed{\\frac{7}{11}}$$\n",
    "\n",
    "$$P(\\text{Windy = Strong} \\ | \\ \\text{NOT PlayTennis}) = \\boxed{\\frac{4}{7}}$$\n",
    "\n",
    "$$P(\\text{Windy = Weak} \\ | \\ \\text{NOT PlayTennis}) = \\boxed{\\frac{3}{7}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: What would your Naive Bayes model predict for the following two weather conditions. Show all work.  \n",
    "- **Forecast**=overcast, **Temp**=cool, **Humidity**=high, **Wind**=weak  \n",
    "- **Forecast**=sunny, **Temp**=hot, **Humidity**=normal, **Wind**=strong  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$$P(\\text{PlayTennis | Forecast = overcast, Temp=cool, Humidity=high, Wind=weak}) = P(\\text{PlayTennis}) \\cdot P(\\text{Forecast = overcast | PlayTennis}) \\cdot P(\\text{Temp = Cool | PlayTennis}) \\cdot P(\\text{Humidity = High | PlayTennis}) \\cdot P(\\text{Wind = Weak | PlayTennis}) $$\n",
    "\n",
    "$$ = \\frac{9}{14} \\cdot \\frac{5}{12} \\cdot \\frac{1}{3} \\cdot \\frac{4}{11} \\cdot \\frac{7}{11} = \\boxed{0.02} $$\n",
    "\n",
    "$$P(\\text{Not PlayTennis | Forecast = overcast, Temp=cool, Humidity=high, Wind=weak}) = P(\\text{Not PlayTennis}) \\cdot P(\\text{Forecast = overcast | Not PlayTennis}) \\cdot P(\\text{Temp = Cool | Not PlayTennis}) \\cdot P(\\text{Humidity = High | Not PlayTennis}) \\cdot P(\\text{Wind = Weak | Not PlayTennis}) $$\n",
    "\n",
    "$$= \\frac{5}{14} \\cdot \\frac{1}{8} \\cdot \\frac{1}{4} \\cdot \\frac{5}{7} \\cdot \\frac{3}{7} = \\boxed{0.003}$$\n",
    "##### So the first one will be classified as Playing Tennis.\n",
    "\n",
    "$$P(\\text{PlayTennis | Forecast = sunny, Temp=hot, Humidity=normal, Wind=strong}) = P(\\text{PlayTennis}) \\cdot P(\\text{Forecast = sunny | PlayTennis}) \\cdot P(\\text{Temp = hot | PlayTennis}) \\cdot P(\\text{Humidity = normal | PlayTennis}) \\cdot P(\\text{Wind = strong | PlayTennis}) $$\n",
    "\n",
    "$$ = \\frac{9}{14} \\cdot \\frac{1}{4} \\cdot \\frac{1}{4} \\cdot \\frac{7}{11} \\cdot \\frac{4}{11} = \\boxed{0.009} $$\n",
    "\n",
    "$$(\\text{Not PlayTennis | Forecast = sunny, Temp=hot, Humidity=normal, Wind=strong}) = P(\\text{Not PlayTennis}) \\cdot P(\\text{Forecast = sunny | Not PlayTennis}) \\cdot P(\\text{Temp = hot | Not PlayTennis}) \\cdot P(\\text{Humidity = normal | Not PlayTennis}) \\cdot P(\\text{Wind = strong | Not PlayTennis})$$\n",
    "\n",
    "$$ = \\frac{5}{14} \\cdot \\frac{1}{2} \\cdot \\frac{3}{8} \\cdot \\frac{2}{7} \\cdot \\frac{4}{7} = \\boxed{0.01} $$\n",
    "\n",
    "##### So the second one will  be classified as Not Playing Tennis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [35 points] Problem 2 - Implementing Discrete Naive Bayes for Text Classification \n",
    "***\n",
    "\n",
    "In this problem you'll implement a general Discrete Naive Bayes class for text classification. Your tasks will be to implement `train`, `predict_log_score`, and `predict` routines to learn the Naive Bayes model parameters from a collection on text and make predictions on unseen validation data. \n",
    "\n",
    "The skeleton for the `TextNB` class is below. Note that this class is fairly similar to the one you worked with in the Hands-On Naive Bayes in-class notebook, so you should look there to remind yourself of the details. Scroll down to find more information about your tasks as well as unit tests.\n",
    "\n",
    "**Important Note**: In Problem 3 we'll be using the `TextNB` class to make predictions about Twitter data.  Since real-world text data typically has a large number of features, you'll want to make your implementation reasonably efficient so that your experiments in Problem 3 don't take forever. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T14:42:09.159492Z",
     "start_time": "2018-04-03T14:42:09.076713Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TextNB:\n",
    "    def __init__(self, text_train, y_train, alpha=1.0):\n",
    "        \"\"\"\n",
    "        :param text_train: a list or ndarray of text strings to use as training data \n",
    "        :param y_train: an ndarray of true labels associated with the text data \n",
    "        :param alpha: the Laplace smoothing parameter \n",
    "        \"\"\"\n",
    "        \n",
    "        # store training data \n",
    "        self.text_train = text_train \n",
    "        self.y_train = y_train \n",
    "        \n",
    "        # store smoothing parameter\n",
    "        self.alpha = alpha \n",
    "        \n",
    "        # get number of classes \n",
    "        self.num_classes = len(set(y_train))\n",
    "        \n",
    "        # initialize vocab to feature map \n",
    "        self.vocab = dict() \n",
    "        \n",
    "        # initialize class counts \n",
    "        self.class_counts = np.zeros(self.num_classes, dtype=int)\n",
    "        \n",
    "        # initialize feature counts (Note, will need to update this with the correct\n",
    "        # number of columns during the training process)\n",
    "        self.feature_counts = np.zeros((self.num_classes, 0), dtype=int)\n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Learn the vocabularly, class_counts, and feature counts from the training data \n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO \n",
    "        v_counter = 0\n",
    "        \n",
    "        # initialize feature counts \n",
    "        self.feature_counts\n",
    "        for ii, sentence in enumerate(self.text_train):\n",
    "            self.class_counts[self.y_train[ii]] += 1\n",
    "            words = sentence.split()\n",
    "            for word in words:\n",
    "                if word not in self.vocab:\n",
    "                    self.vocab[word] = v_counter\n",
    "                    v_counter += 1\n",
    "        \n",
    "        self.feature_counts = np.zeros((self.num_classes, len(self.vocab)), dtype=int)\n",
    "        \n",
    "        for ii, sentence in enumerate(self.text_train):\n",
    "            words = sentence.split()\n",
    "            for word in words:\n",
    "                self.feature_counts[self.y_train[ii]][self.vocab[word]] += 1\n",
    "            \n",
    "                    \n",
    "    def predict_log_score(self, text_str):\n",
    "        \"\"\"\n",
    "        Get the log-probability score for each class\n",
    "        for a query string\n",
    "        \n",
    "        :param text_str: a single string of text to compute the log_score for \n",
    "        \"\"\"\n",
    "        \n",
    "        class_scores = np.zeros(self.num_classes) \n",
    "        word_array = text_str.split()\n",
    "        for c in range(self.num_classes):\n",
    "            class_scores[c] = np.log((self.class_counts[c] + self.alpha) / (np.sum(self.class_counts) + (self.alpha*self.num_classes)))\n",
    "            for word in word_array:\n",
    "                if word not in self.vocab:\n",
    "                    continue\n",
    "                else:\n",
    "                    class_scores[c] += np.log((self.feature_counts[c, self.vocab[word]] + self.alpha) / (np.sum(self.feature_counts[c]) + len(self.vocab)))\n",
    "        \n",
    "        return class_scores\n",
    "        \n",
    "    \n",
    "    def predict(self, text_list):\n",
    "        \"\"\"\n",
    "        Predict the class of each example in text_list  \n",
    "        \n",
    "        :param text_list: a list or ndarray of text strings to make predictions on \n",
    "        \"\"\"\n",
    "        yhat = np.zeros(len(text_list), dtype=int)\n",
    "        for ii,text_str in enumerate(text_list):\n",
    "            yhat[ii] = np.argmax(self.predict_log_score(text_str))\n",
    "        \n",
    "        return yhat \n",
    "        \n",
    "        \n",
    "    def accuracy(self, text_list, y_true):\n",
    "        \"\"\"\n",
    "        Make predictions on texts in text_list and compute accuracy relative to \n",
    "        true labels in y_true \n",
    "        \n",
    "        :param text_list: a list or ndarray of text strings to make predictions on \n",
    "        :param y_list: an ndarray of true labels associated with the text data \n",
    "        \"\"\"\n",
    "        yhat = self.predict(text_list)\n",
    "        return np.sum(yhat == y_true)/len(y_true)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: Complete the `train` function in the `TextNB` class to prepare to make Naive Bayes predictions using Laplace smoothing.  In this routine you will need to populate the following data structures: \n",
    "\n",
    "`self.vocab`: A Python dictionary that maps distinct terms found in the training set to unique indices in $\\{0, 1, \\ldots, |V|-1\\}$.  This will allow us to quickly look up frequency counts for an encountered term in a Numpy array. Note that while the data is fairly clean (We've removed punctuation and made all characters lowercase) you should take care that you're not accidentally counting whitespace in the vocabulary. \n",
    "\n",
    "`self.class_counts`: A 1D Numpy array of length `self.num_classes` which counts the number of documents in the training set that belong to each class. \n",
    "\n",
    "`self.feature_counts`: A 2D Numpy array of dimensions `self.num_classes` $\\times$ $|V|$. The $(c,k)$-entry in this array should be the number of times that term $k$ appears in documents belonging to class $c$. Note that we're using the Bag-of-Words approach here, so if a term appears multiple times in a single document, each instance of that term should be counted.  \n",
    "\n",
    "When you think you're done, execute the following unit test, corresponding to the example starting on Slide 29 of [Lecture 24](https://www.cs.colorado.edu/~ketelsen/files/courses/csci4622/slides/lesson24.pdf). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T14:45:46.325834Z",
     "start_time": "2018-04-03T14:45:46.314267Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testVocab (__main__.TestNB) ... ok\n",
      "testClassCounts (__main__.TestNB) ... ok\n",
      "testFeatureCounts (__main__.TestNB) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.003s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "%run -i tests/tests.py \"prob 2A\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Complete the `predict_log_score` function in the `TextNB` class to take in a single string of text and compute the associated log-score for each class. For now, you should use add-one Laplace smoothing for both the class-priors and the class-conditional probabilities.  In **Problem 3** we'll experiment with different variants of Laplace smoothing, so if you like you can read ahead now and implement the general version of Laplace smoothing from the beginning.  \n",
    "\n",
    "**Note**: For simplicity and testing purposes, do not implement an `UNK` feature.  Instead, if you encounter a term not in the vocabulary you can safely ignore it. \n",
    "\n",
    "When you think your `predict_log_score` function is working well, execute the following unit test. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T14:45:47.998683Z",
     "start_time": "2018-04-03T14:45:47.990764Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testLogScore (__main__.TestNB) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.001s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "%run -i tests/tests.py \"prob 2B\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Finally, implement the `predict` method to take in a list or ndarray of text data, call `predict_log_score`, and return a vector of predicted labels. \n",
    "\n",
    "When you think you're done, execute the following unit test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T14:45:49.228361Z",
     "start_time": "2018-04-03T14:45:49.220931Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testPredict (__main__.TestNB) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.002s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "%run -i tests/tests.py \"prob 2C\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [30 points] Problem 3: Predicting the Sentiment of Tweets sent from Passengers to Airlines \n",
    "***\n",
    "\n",
    "In this problem you'll use the `TextNB` class you wrote in **Problem 2** to make predictions about the sentiment of tweets sent by passengers to airlines.  Execute the following cell to load the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T14:45:55.395606Z",
     "start_time": "2018-04-03T14:45:55.371833Z"
    }
   },
   "outputs": [],
   "source": [
    "f = open('data/airline_tweets.pklz','rb')\n",
    "text_train, y_train, text_valid, y_valid, text_all, y_all = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: Explore the data and answer the following questions: \n",
    "\n",
    "- How many total examples are there in the training and validation sets? \n",
    "- Which binary label ($\\{0,1\\}$) corresponds to tweets with positive and negative sentiment, respectively?\n",
    "- What percentage of tweets in the training set have true positive sentiment? \n",
    "- What percentage of tweets in the validation set have true positive sentiment? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total examples in the training set is: 4000\n",
      "total examples in the validation set is: 4000\n",
      "\n",
      "@southwestair awesome  thanks has label 1\n",
      "Label y=1 correlates to positive sentiment and y=0 correlates to negative sentiment.\n",
      "\n",
      "0.25% of tweets in training set have true positive sentiment.\n",
      "0.23075% of tweets in validation set have true positive sentiment.\n"
     ]
    }
   ],
   "source": [
    "print('total examples in the training set is: ' + str(text_train.shape[0]))\n",
    "print('total examples in the validation set is: ' + str(text_valid.shape[0]))\n",
    "print('\\n' + text_train[0], 'has label', y_train[0])\n",
    "print('Label y=1 correlates to positive sentiment and y=0 correlates to negative sentiment.')\n",
    "print('\\n' + str(np.sum(y_train)/len(y_train)) + '% of tweets in training set have true positive sentiment.')\n",
    "print(str(np.sum(y_valid)/len(y_valid)) + '% of tweets in validation set have true positive sentiment.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Use your `TextNB` class to learn a Naive Bayes classifier for the airline Twitter data.  What accuracy do you achieve on the training set and what accuracy do you achieve on the test set? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlineNB = TextNB(text_train, y_train)\n",
    "airlineNB.train()\n",
    "training_accuracy = airlineNB.accuracy(text_train, y_train)\n",
    "valid_accuracy = airlineNB.accuracy(text_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy is 93.05%\n",
      "Valid Accuracy is 91.075%\n"
     ]
    }
   ],
   "source": [
    "print('Training Accuracy is ' + str(round(training_accuracy * 100, 3)) + '%')\n",
    "print('Valid Accuracy is ' + str(round(valid_accuracy * 100, 3)) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Notice that if we want to make improvements in our Naive Bayes classifier, we don't really have a lot of knobs to turn aside from changing the word features that we use.  One place that we might make some gains though is in using a slightly different version of Laplace smoothing.  \n",
    "\n",
    "Recall that in add-one smoothing we add a $1$ to the numerator in both the estimation of the prior probabilities and the class-conditional likelihoods. \n",
    "\n",
    "$$\n",
    "\\hat{p}(\\textrm{Class}) = \\dfrac{\\textrm{# docs from Class}+1}{\\textrm{# total docs in training data} + |C|},\n",
    "\\quad \\quad\n",
    "\\hat{p}(\\textrm{term} \\mid \\textrm{Class}) = \\dfrac{\\textrm{# instance of term in Class}+1}{\\textrm{# total words in Class} + |V|}\n",
    "$$\n",
    "\n",
    "It turns out there's nothing sacred about adding $1$ to the numerators.  In fact, we can add any positive value $\\alpha$ that we like \n",
    "\n",
    "$$\n",
    "\\hat{p}(\\textrm{Class}) = \\dfrac{\\textrm{# docs from Class}+\\alpha}{\\textrm{# total docs in training data} + ?},\n",
    "\\quad \\quad\n",
    "\\hat{p}(\\textrm{term} \\mid \\textrm{Class}) = \\dfrac{\\textrm{# instance of term in Class}+\\alpha}{\\textrm{# total words in Class} + ?}\n",
    "$$\n",
    "\n",
    "Explain what modification must be made to the denominators so that theses estimates remain valid probabilities. Clearly justify your reasoning. \n",
    "\n",
    "Support this modification in your `TextNB` class above, if you have not already.  Make sure that your code still passes then unit tests when $\\alpha = 1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Filling in the blanks, suppose alpha is a value we use as our laplace smoothing factor for our classifier, then when in general form (including when alpha != 1) we get this general form: \n",
    "$$\\hat{p}(\\textrm{Class}) = \\dfrac{\\textrm{# docs from Class}+\\alpha}{\\textrm{# total docs in training data} + \\alpha \\cdot |C|},\n",
    "\\quad \\quad\n",
    "\\hat{p}(\\textrm{term} \\mid \\textrm{Class}) = \\dfrac{\\textrm{# instance of term in Class}+\\alpha}{\\textrm{# total words in Class} + \\alpha \\cdot |V|}$$\n",
    "\n",
    "#### We must multiply the cardinality by $\\alpha$ to ensure the probabilities will still add up to one. So now not if an $\\alpha$ value of less than one or more than one is used, then the probabilities sum to one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: Write some code to perform $K$-Folds cross-validation on the entire data set (`train_all` and `y_all`) to estimate the accuracy of your NB classifier for various values of $\\alpha$ and make a plot showing your results.  \n",
    "\n",
    "To do the partitioning into folds I recommend leveraging sklearn's [StratifiedKFold]() routine.  The documentation demonstrates how it can be used.  \n",
    "\n",
    "For your plot, use at least $K=5$ folds and at least $5$ different values of $\\alpha$ between $0.1$ and $1.5$.  Which value of $\\alpha$ seems to perform the best? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On Alpha=0.1\n",
      "On Alpha=0.256\n",
      "On Alpha=0.411\n",
      "On Alpha=0.567\n",
      "On Alpha=0.722\n",
      "On Alpha=0.878\n",
      "On Alpha=1.033\n",
      "On Alpha=1.189\n",
      "On Alpha=1.344\n",
      "On Alpha=1.5\n",
      "[0.9097519252448928, 0.911626847754628, 0.9127516134283645, 0.913252238526265, 0.9126270039070328, 0.9107518468757213, 0.9100030185070385, 0.9095028620616649, 0.9080027054209786, 0.9061280954113654]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113e28860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "K = 5\n",
    "alphas = np.linspace(0.1, 1.5, 10)\n",
    "skf = StratifiedKFold(n_splits=K)\n",
    "\n",
    "test_accuracies = []\n",
    "for alpha in alphas:\n",
    "    averages = []\n",
    "    print('On Alpha=' + str(round(alpha, 3)))\n",
    "    for train_index, test_index in skf.split(text_all, y_all):\n",
    "#         print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = text_all[train_index], text_all[test_index]\n",
    "        y_train, y_test = y_all[train_index], y_all[test_index]\n",
    "\n",
    "        fold_NB = TextNB(X_train, y_train, alpha)\n",
    "        fold_NB.train()\n",
    "        averages.append(fold_NB.accuracy(X_test, y_test))\n",
    "        \n",
    "    test_accuracies.append(np.mean(averages))\n",
    "    \n",
    "print(test_accuracies)\n",
    "plt.plot(alphas, test_accuracies)\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('Valid_Set_Accuracy %')\n",
    "plt.title('Alpha vs. Valid_Set_Accuracy')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The best value of $\\alpha$ appears to be between right around $\\textbf{~0.55}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [20 points] Problem 4: VC Dimension \n",
    "***\n",
    "\n",
    "**Part A**: Consider learning to classify binary labeled data with a single feature $x$.  Let $H$ be the hypothesis class described by the union of two intervals $[a,b] \\cup [c,d]$ such that $h(x)$ labels an example as positive if it's in the interval $[a,b]$ **OR** the interval $[c,d]$.  Determine the VC Dimension of $H$.  Justify your conclusion by demonstrating a shattering of a set $S$ of the appropriate size **AND** by arguing that an arbitrary set consisting of one additional point cannot be shattered by $H$.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### The VC Dimension of H is 4. Notice that every combination of 4 points and their colors (red and blue) can be separeted and correctly classified or also known as Shattered. Once a 5th point is added, it becomes impossible to shattered the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://lh3.googleusercontent.com/_06g4gTFOGdHUZRc1qM9aLX3fXfzU23FQ5ejlnn8t7I2V64dJRjG0yWszyZdy1fnMG8MndTFzH6UeNEujCWrsc8TidFsEQh_gbXdZAwoan_xfIZd4MGN7k5Wr5t68xCOATUisYsyEeM=w2400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url='https://lh3.googleusercontent.com/_06g4gTFOGdHUZRc1qM9aLX3fXfzU23FQ5ejlnn8t7I2V64dJRjG0yWszyZdy1fnMG8MndTFzH6UeNEujCWrsc8TidFsEQh_gbXdZAwoan_xfIZd4MGN7k5Wr5t68xCOATUisYsyEeM=w2400')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://lh3.googleusercontent.com/yC-EYLcz4rvXiIgqIvxKolRy2GN0qddoufwIb2PSKCjuatoTu5IYG2jj6dTXP7rkaFYYmox0dJ8uQn8pVHTjl88CPq3xq5d7tm5fzodfV1oqyAjUmyxBvbfo7qMfpxwLPlOsxvSiwRM=w2400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://lh3.googleusercontent.com/yC-EYLcz4rvXiIgqIvxKolRy2GN0qddoufwIb2PSKCjuatoTu5IYG2jj6dTXP7rkaFYYmox0dJ8uQn8pVHTjl88CPq3xq5d7tm5fzodfV1oqyAjUmyxBvbfo7qMfpxwLPlOsxvSiwRM=w2400')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://lh3.googleusercontent.com/wOKSrDFv6eP9VFj6sQdukF-Q3MijNNn45TpCuYwry17q3oWKZw-HFwKlmxFyhE789LmsSzLLNYaMqmpZKZROsdZmpo6vNARo2-ovkYYIwTfCILgmRul3Z_G9gX9cDwypMEfvlkMT1tA=w2400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://lh3.googleusercontent.com/wOKSrDFv6eP9VFj6sQdukF-Q3MijNNn45TpCuYwry17q3oWKZw-HFwKlmxFyhE789LmsSzLLNYaMqmpZKZROsdZmpo6vNARo2-ovkYYIwTfCILgmRul3Z_G9gX9cDwypMEfvlkMT1tA=w2400')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notice that when adding an additional 5th point, it is now impossible to shatter the set since the blue point on the right make it impossible to correctly classify the red points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://lh3.googleusercontent.com/tvHMhR1lMOfvV-_j5Nop4DvTT1y9abRJier0PeYVj0G7WiJHMneWQ7ZWMfcT0aXUjKPGmsr33ukSaaOgwB6mUhpCdiTFwPuF22SDQAUn1B9oFVissrh1s-gnjJ6t7US7xpDHsL5C4U0=w2400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://lh3.googleusercontent.com/tvHMhR1lMOfvV-_j5Nop4DvTT1y9abRJier0PeYVj0G7WiJHMneWQ7ZWMfcT0aXUjKPGmsr33ukSaaOgwB6mUhpCdiTFwPuF22SDQAUn1B9oFVissrh1s-gnjJ6t7US7xpDHsL5C4U0=w2400')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Consider learning to classify binary labeled data with two features, $x_1$ and $x_2$.  Let $H$ be the hypothesis class described by the ability to assign all points in a particular quadrant of the 2D plane to be positive or negative, with the restriction that at least one of the four quadrants must be labeled positive and at least one of the four quadrants must be labeled negative. Determine the VC Dimension of $H$. Again, justify your conclusion by demonstrating a shattering of a set $S$ of the appropriate size **AND** by arguing that an arbitrary set consisting of one additional point cannot be shattered by $H$.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### I will argue that the VC Dimension of H is 4. Notice that for every combination of colors for the four points spread out in each quadrant, it is possible to shatter the set of points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://lh3.googleusercontent.com/BX20mU5ItVmOGfk4DZOnk_ckOqBw6yWej41sqZ8P3PrNTMBZhWNnjKvp5O7EB9D6C2bxaI6AFk76ERFveDwvVziPDdxGHgRoG7AkzrAPKzWyhC0fsM52MMczNR7xTUb3Uox4D5qjGu4=w2400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://lh3.googleusercontent.com/BX20mU5ItVmOGfk4DZOnk_ckOqBw6yWej41sqZ8P3PrNTMBZhWNnjKvp5O7EB9D6C2bxaI6AFk76ERFveDwvVziPDdxGHgRoG7AkzrAPKzWyhC0fsM52MMczNR7xTUb3Uox4D5qjGu4=w2400')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://lh3.googleusercontent.com/V5jg7DdJ6hHtILMdK63dbR2iyv95n0UqkM6xv8zOHz4ri-F8LwMwcQ-S7YwFgmZEsg88S5Lk_v93Z_QvftXS-U2ggEY-B4hpGtb73YquIETIChKnhPsnDSNrSXY1LY8JJ047OhXiJ-I=w2400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://lh3.googleusercontent.com/V5jg7DdJ6hHtILMdK63dbR2iyv95n0UqkM6xv8zOHz4ri-F8LwMwcQ-S7YwFgmZEsg88S5Lk_v93Z_QvftXS-U2ggEY-B4hpGtb73YquIETIChKnhPsnDSNrSXY1LY8JJ047OhXiJ-I=w2400')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://lh3.googleusercontent.com/X6J5x34Qcd8XrmUHalxuXfeb3NiceM1QjCWrQFrA24wrMQwZQtvUDC_i7GuT2bNakf6yz1Hr8YBgXtiX8fIx2D9bERgX_uJIzI0rHxTtO1tqd_pqH13WUJb3WiIuBQoBlCc4vdr5LPI=w2400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://lh3.googleusercontent.com/X6J5x34Qcd8XrmUHalxuXfeb3NiceM1QjCWrQFrA24wrMQwZQtvUDC_i7GuT2bNakf6yz1Hr8YBgXtiX8fIx2D9bERgX_uJIzI0rHxTtO1tqd_pqH13WUJb3WiIuBQoBlCc4vdr5LPI=w2400')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### However, when adding one more additional point, it becomes impossible to shatter the set of points. Because now there are two points that will be in the same quadrant and if they are different colors, then you cannot correctly classify all the points in that quadrant. Therefore, the VCDim = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://lh3.googleusercontent.com/j1xUCRsGhSI0Fnm9kbJIiPivFFwFyfh7I5vh-x1wW82qKr0In6GvsrFI4qAWWZCBwHYtM8-NP4Y7JqBtL5wxI2OamzEjtKtgArV_k3ObWSUQirQVLV6rWGqV62rP_jVvHH1J4Snramk=w2400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://lh3.googleusercontent.com/j1xUCRsGhSI0Fnm9kbJIiPivFFwFyfh7I5vh-x1wW82qKr0In6GvsrFI4qAWWZCBwHYtM8-NP4Y7JqBtL5wxI2OamzEjtKtgArV_k3ObWSUQirQVLV6rWGqV62rP_jVvHH1J4Snramk=w2400')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
