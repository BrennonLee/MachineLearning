{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2: K-Nearest Neighbors and The Perceptron Algorithm \n",
    "***\n",
    "\n",
    "**Name**: Brennon Lee\n",
    "\n",
    "***\n",
    "\n",
    "This assignment is due on Moodle by **5pm on Friday February 16th**. Your solutions to theoretical questions should be done in Markdown/MathJax directly below the associated question.  Your solutions to computational questions should include any specified Python code and results as well as written commentary on your conclusions.  Remember that you are encouraged to discuss the problems with your instructors and classmates, but **you must write all code and solutions on your own**.  For a refresher on the course **Collaboration Policy** click [here](https://github.com/chrisketelsen/CSCI-4622-Machine-Learning/blob/master/resources/syllabus.md#collaboration-policy).\n",
    "\n",
    "**NOTES**: \n",
    "\n",
    "- Do **NOT** load or use any Python packages that are not available in Anaconda 3.6. \n",
    "- Some problems with code may be autograded.  If we provide a function API **do not** change it.  If we do not provide a function API then you're free to structure your code however you like. \n",
    "- Submit only this Jupyter notebook to Moodle.  Do not compress it using tar, rar, zip, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle, gzip\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [15 points] Problem 1 - Exploring Unweighted and Distance-Weighted KNN\n",
    "***\n",
    "\n",
    "In class we talked in detail about the standard Unweighted K-Nearest Neighbors classifier and touched briefly on the so called Distance-Weighted KNN classifier.  In this problem you'll get some more practice working with each. \n",
    "\n",
    "Consider the data set shown below where red dots correspond to training examples with label $y=1$ and blue dots correspond to training examples with label $y = -1$. The green point ${\\bf x}_G$ and orange point ${\\bf x}_O$ are query points that we would like to classify.  \n",
    "\n",
    "![KNN Data](figs/prob1.png \"Prob 1 Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: How would a $6$-NN classifier classify the point ${\\bf x}_G$? Fully justify your response. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Using the 6 nearest neighbors, the point $X_{G}$ is going to be classified as a blue dot ($y=-1$) because of the 6 points closest to it, $\\frac{4}{6} = $ blue and $\\frac{2}{6}$ = red. Majority of the points are blue so that is what we will predict $X_{G}$ to be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Use the probabilistic interpretation of KNN to estimate the probabilities $p(y_G = 1 \\mid {\\bf x}_G)$ and $p(y_G = -1 \\mid {\\bf x}_G)$. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### (Number of red dots within 6 NN) $$p(y_{G} = 1 \\ | \\ x_{G}) \\ = \\frac{2}{6}$$\n",
    "\n",
    "##### (Number of blue dots within 6 NN) $$p(y_{G} = -1 \\ | \\ x_{G}) \\ = \\frac{4}{6}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: The so-called Distance-Weighted KNN classifier assigns weights to the the nearest-neighbor training examples proportional to the inverse-distance from the training example to the query point.  Classification is performed by summing the weights associated with each class and predicting the class with the highest weighted-majority vote.  Mathematically we might describe the weighted-vote for a class $c$ as \n",
    "\n",
    "$$\n",
    "\\textrm{Weighted-Vote}(c) = \\displaystyle\\sum_{i \\in {\\cal N}_K} I(y_i = c) \\times \\dfrac{1}{\\|{\\bf x}_i - {\\bf x}\\|}\n",
    "$$\n",
    "\n",
    "See Slides 40-43 of the [KNN Lecture](https://www.cs.colorado.edu/~ketelsen/files/courses/csci4622/slides/lesson06.pdf).  For a worked example. \n",
    "\n",
    "Use the Distance-Weighted 6-NN classifier to classify the point ${\\bf x}_G$ above. Show your work. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Blue Point Distances:\n",
    "$$(\\sqrt{2},\\sqrt{2},\\sqrt{2},\\sqrt{2})$$ \n",
    "##### Blue Weighted Majority Vote\n",
    "$$\\frac{1}{\\sqrt{2}} + \\frac{1}{\\sqrt{2}} + \\frac{1}{\\sqrt{2}}+ \\frac{1}{\\sqrt{2}} = \\boxed{2\\sqrt{2}}$$\n",
    "##### Red Point Distances:\n",
    "$$(1, 0.5)$$ \n",
    "##### Red Weighted Majority Vote\n",
    "$$\\frac{1}{1} + \\frac{1}{0.5} = \\boxed{3}$$\n",
    "\n",
    "#####  By Distance-Weighted KNN, the classification of point $X_{G}$ would be Red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: Where could you place an additional blue training example such that the Distance-Weighted 5-NN classifier would encounter a tie between red and blue when classifying the point ${\\bf x}_O$?  Justify your response. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### To get a tie between red and blue for 5-NN, a blue training point would have to be placed at a distance of $\\boxed{\\frac{1}{\\sqrt{2}}}$ away from $X_{O}$. This is because the red weighted distance is: $\\frac{1}{\\sqrt{2}} + \\frac{1}{\\sqrt{2}} + \\frac{1}{\\sqrt{2}} = \\frac{3}{\\sqrt{2}}$. With an additional blue point $\\frac{1}{\\sqrt{2}}$ away, the blue weighted distance becomes: $\\frac{1}{\\sqrt{2}} + \\sqrt{2}  = \\frac{3}{\\sqrt{2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part E**: State a general formula for the probability $p(Y=c \\mid {\\bf x})$ using the Distance-Weighted KNN classifier.  Use your formula to estimate the probabilities $p(y_G = 1 \\mid {\\bf x}_G)$ and $p(y_G = -1 \\mid {\\bf x}_G)$ for Distance-Weighted 6-NN.  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### The general formula can be written as:\n",
    "$$p(Y = c \\ | \\ x) = \\frac{\\displaystyle\\sum_{i \\in {\\cal N}_k} I(y_i = c) \\times \\dfrac{1}{\\|{\\bf x}_i - {\\bf x}\\|}}{\\displaystyle\\sum_{i \\in {\\cal N}_k}\\ \\dfrac{1}{\\|{\\bf x}_i - {\\bf x}\\|}}$$\n",
    "##### Where the numerator is just the weighted-vote function given to us and the denominator represents the sum of all the K nearest neighbor weights. The result gives us the probabilty we are looking for given our point X. \n",
    "\n",
    "##### For $p(y_{G} = 1 \\ | \\ x_{G})$ the equation for red dots can be written as:\n",
    "$$p(y_{G} = 1 \\ | \\ x_{G}) = \\frac{\\displaystyle\\sum_{i \\in {\\cal N}_6} I(y_i = 1) \\times \\dfrac{1}{\\|{\\bf x}_i - {\\bf x}\\|}}{\\displaystyle\\sum_{i \\in {\\cal N}_6}\\ \\dfrac{1}{\\|{\\bf x}_i - {\\bf x}\\|}}$$\n",
    "\n",
    "$$\\frac{(1 \\cdot \\frac{1}{0.5}) + (1 \\cdot \\frac{1}{1}) + (0 \\cdot \\frac{1}{\\sqrt{2}}) + (0 \\cdot \\frac{1}{\\sqrt{2}}) + (0 \\cdot \\frac{1}{\\sqrt{2}}) + (0 \\cdot \\frac{1}{\\sqrt{2}})}{\\frac{1}{0.5} + \\frac{1}{1} + \\frac{1}{\\sqrt{2}} + \\frac{1}{\\sqrt{2}} + \\frac{1}{\\sqrt{2}} + \\frac{1}{\\sqrt{2}}} = \\boxed{\\frac{3}{3 + 2\\sqrt{2}}}$$\n",
    "\n",
    "##### For $p(y_{G} = -1 \\ | \\ x_{G})$ the equation for blue dots can be written as:\n",
    "$$p(y_{G} = -1 \\ | \\ x_{G}) = \\frac{\\displaystyle\\sum_{i \\in {\\cal N}_6} I(y_i = -1) \\times \\dfrac{1}{\\|{\\bf x}_i - {\\bf x}\\|}}{6}$$\n",
    "\n",
    "$$\\frac{(0 \\cdot \\frac{1}{0.5}) + (0 \\cdot \\frac{1}{1}) + (1 \\cdot \\frac{1}{2}) + (1 \\cdot \\frac{1}{2}) + (1 \\cdot \\frac{1}{2}) + (1 \\cdot \\frac{1}{2})}{\\frac{1}{0.5} + \\frac{1}{1} + \\frac{1}{\\sqrt{2}} + \\frac{1}{\\sqrt{2}} + \\frac{1}{\\sqrt{2}} + \\frac{1}{\\sqrt{2}}} = \\boxed{\\frac{2\\sqrt{2}}{3 + 2\\sqrt{2}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [35 points] Problem 2- KNN for Handwritten Digit Recognition \n",
    "***\n",
    "\n",
    "In this problem you'll implement a K-Nearest Neighbor framework to take an image of a handwritten digit and predict which digit it corresponds to.  \n",
    "\n",
    "![Samples of Handwritten Digits](figs/mnist.png \"MNIST Digits\")\n",
    "\n",
    "To keep run times down we'll only consider the subset of the MNIST data set consisting of the digits $3, 7, 8$ and $9$. \n",
    "\n",
    "**Part A**: Executing the following cells will load training and validation data and plot an example handwritten digit.  Explore the training and validation sets and answer the following questions: \n",
    "\n",
    "- How many pixels are in each image in the data set?  \n",
    "- How many examples are there from each class in the training set? \n",
    "- How many examples are there from each class in the validation set? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_valid, y_valid = pickle.load(gzip.open(\"data/mnist21x21_3789.pklz\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_digit(x, label=None):\n",
    "    fig = plt.figure(figsize=(3,3))\n",
    "    plt.imshow(x.reshape(21,21), cmap='gray');\n",
    "    plt.xticks([]); plt.yticks([]);\n",
    "    if label: plt.xlabel(\"true: {}\".format(label), fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1119091d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_index = 0\n",
    "view_digit(X_train[training_index], y_train[training_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are (441,) pixels in each image set.\n",
      "\n",
      "There are 1000 examples from class 3 in the training set.\n",
      "There are 1000 examples from class 7 in the training set.\n",
      "There are 1000 examples from class 8 in the training set.\n",
      "There are 1000 examples from class 9 in the training set.\n",
      "\n",
      "There are 250 examples from class 3 in the validation set.\n",
      "There are 250 examples from class 7 in the validation set.\n",
      "There are 250 examples from class 8 in the validation set.\n",
      "There are 250 examples from class 9 in the validation set.\n"
     ]
    }
   ],
   "source": [
    "print('There are ' + str(X_train[0].shape[0]) + ' pixels in each image set.')\n",
    "print('\\nThere are ' + str(sum(y_train == 3)) + ' examples from class 3 in the training set.')\n",
    "print('There are ' + str(sum(y_train == 7)) + ' examples from class 7 in the training set.')\n",
    "print('There are ' + str(sum(y_train == 8)) + ' examples from class 8 in the training set.')\n",
    "print('There are ' + str(sum(y_train == 9)) + ' examples from class 9 in the training set.')\n",
    "\n",
    "print('\\nThere are ' + str(sum(y_valid == 3)) + ' examples from class 3 in the validation set.')\n",
    "print('There are ' + str(sum(y_valid == 7)) + ' examples from class 7 in the validation set.')\n",
    "print('There are ' + str(sum(y_valid == 8)) + ' examples from class 8 in the validation set.')\n",
    "print('There are ' + str(sum(y_valid == 9)) + ' examples from class 9 in the validation set.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next part of this problem will involve building a `KNN` class for doing K-Nearest-Neighbors classification. Scroll down to **Part B** to see what you need to do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class KNN:\n",
    "    \"\"\"\n",
    "    Class to store data for regression problems \n",
    "    \"\"\"\n",
    "    def __init__(self, X_train, y_train, K=5, distance_weighted=False):\n",
    "        \"\"\"\n",
    "        Creates a kNN instance\n",
    "\n",
    "        :param X_train: Training data input in 2D ndarray \n",
    "        :param y_train: Training data output in 1D ndarray \n",
    "        :param K: The number of nearest points to consider in classification\n",
    "        :param distance_weighted: Bool indicating whether to use distance weighting\n",
    "        \"\"\"\n",
    "        \n",
    "        # Import and build the BallTree on training features \n",
    "        from sklearn.neighbors import BallTree\n",
    "        self.balltree = BallTree(X_train)\n",
    "        \n",
    "        # Cache training labels and parameter K \n",
    "        self.y_train = y_train\n",
    "        self.K = K \n",
    "        \n",
    "        # Boolean flag indicating whether to do distance weighting \n",
    "        self.distance_weighted = distance_weighted\n",
    "        \n",
    "    def majority(self, neighbor_indices, neighbor_distances=None):\n",
    "        \"\"\"\n",
    "        Given indices of nearest neighbors in training set, return the majority label. \n",
    "        Break ties by considering 1 fewer neighbor until a clear winner is found. \n",
    "\n",
    "        :param neighbor_indices: The indices of the K nearest neighbors in self.X_train \n",
    "        :param neighbor_distances: Corresponding distances from query point to K nearest neighbors. \n",
    "        \"\"\"\n",
    "       \n",
    "        return 0\n",
    "        \n",
    "    def classify(self, x):\n",
    "        \"\"\"\n",
    "        Given a query point, return the predicted label \n",
    "        \n",
    "        :param x: a query point stored as an ndarray  \n",
    "        \"\"\"\n",
    "        \n",
    "        return self.majority([0,1,2])\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Given an ndarray of query points, return yhat, an ndarray of predictions \n",
    "\n",
    "        :param X: an (m x p) dimension ndarray of points to predict labels for \n",
    "        \"\"\"\n",
    "        return np.zeros(X.shape[0], dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Modify the class above to implement an Unweighted KNN classifier.  There are three methods that you need to complete: \n",
    "\n",
    "- `predict`: Given an $m \\times p$ matrix of validation data with $m$ examples each with $p$ features, return a length-$m$ vector of predicted labels by calling the `classify` function on each example. \n",
    "- `classify`: Given a single query example with $p$ features, return its predicted class label as an integer using KNN by calling the `majority` function. \n",
    "- `majority`: Given an array of indices into the training set corresponding to the $K$ training examples that are nearest to the query point, return the majority label as an integer.  If there is a tie for the majority label using $K$ nearest neighbors, reduce $K$ by 1 and try again.  Continue reducing $K$ until there is a winning label. \n",
    "\n",
    "**Notes**: \n",
    "- Don't even think about implementing nearest-neighbor search or any distance metrics yourself.  Instead, go read the documentation for Scikit-Learn's [BallTree](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html) object.  You will find that its implemented [query](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree.query) method can do most of the heavy lifting for you. \n",
    "- Do not use Scikit-Learn's KNeighborsClassifier in this problem.  We're implementing this ourselves. \n",
    "- You don't need to worry about the `distance_weighted` flag until **Part C**, but we recommend reading ahead a bit. It might be good to think about your implementation of **Part C** before implementing **Part B**. \n",
    "- When you think you're done, execute the following cell to run 4 unit tests based on the example starting on Slide 24 of the [KNN Lecture](https://www.cs.colorado.edu/~ketelsen/files/courses/csci4622/slides/lesson06.pdf).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i tests/tests.py \"prob 2A\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Modify the `KNN` class to perform the distance-weighted KNN classification described in **Problem 1** when the `distance_weighted` flag is set to `True`. A word of caution: it's certainly possible that a query point could be distance $0$ away from some training example.  If this happens your implementation should handle it gracefully and return the appropriate class label.   \n",
    "\n",
    "When you think you're done, execute the following cell to run three final unit tests corresponding to the example on Slide 43 of the [KNN Lecture](https://www.cs.colorado.edu/~ketelsen/files/courses/csci4622/slides/lesson06.pdf). Make sure that the changes you make in **Part C** do not affect the unit tests from **Part B**.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i tests/tests.py \"prob 2B\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: Use your `KNN` class to perform Unweighted KNN on the validation data with $K=3$ and do the following: \n",
    "\n",
    "- Create a **confusion matrix** (feel free to use the Scikit-Learn `confusion_matrix` function described in the [Hands-On KNN](https://github.com/chrisketelsen/CSCI-4622-Machine-Learning/blob/master/in-class-notebooks/Hands_On_KNN_Perceptron.ipynb) from class).  \n",
    "- Based on your confusion matrix, which digits seem to get confused with other digits the most? \n",
    "- Find one misclassified validation example and plot it with the `view_digit` function along with plots of its three nearest neighbors in the training set. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part E**: Create a plot of the accuracy of both Unweighted and Distance-Weighted KNN on the validation set on the same set of axes for $K=1, 2, \\ldots, 20$ (feel free to go out to $K=30$ if your implementation is efficient enough to allow it).  Answer the following questions: \n",
    "\n",
    "- For general $K$, does Unweighted or Weighted KNN appear to perform better? \n",
    "- Which value of $K$ attains the best accuracy on the validation set? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [15 points] Problem 3 - Perceptron Learning By Hand \n",
    "***\n",
    "\n",
    "Consider the following two-feature training set \n",
    "\n",
    "$$\n",
    "\\{({\\bf x}_1 = (1,1), y_1 = 1),~({\\bf x}_2 = (1,-1), y_2 =-1),~({\\bf x}_3 = (-1,-1), y_3 = 1),~({\\bf x}_4 = (-1,1), y_4 = -1)\\}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**:  Perform one pass of the Perceptron Algorithm by hand in the order $1,~2,~3,~4$ starting with the initial weights and bias ${\\bf w} = (0,1)$ and $b = 0$.  Show each step in the Perceptron Algorithm and each intermediate set of weights in Markdown. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**:  What label would your final Perceptron classifier predict for each of the training points? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**:  Is it possible that your Perceptron classifier would ever perfectly classify all training examples after more passes of the Perceptron Algorithm?  Clearly explain your reasoning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [35 points] Problem 4 - The Margin and Convergence of the Perceptron Learning Algorithm \n",
    "***\n",
    "\n",
    "In this problem you will implement the Perceptron Learning Algorithm and use it to explore the convergence of the algorithm on linearly separable simulated data sets with particular properties.  Take a look at the `Perceptron` class below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    \"\"\"\n",
    "    Class to fit a perceptron classifier to simulated data \n",
    "    \"\"\"\n",
    "    def __init__(self, n=100, margin=0.1, random_state=1241, X=None, y=None):\n",
    "        \"\"\"\n",
    "        Initializes Perceptron class.  Generates training data and sets parameters. \n",
    "        \n",
    "        :param n: the number of training examples\n",
    "        :param margin: the margin between decision boundary and data\n",
    "        :param random_state: seed for random number generator \n",
    "        :param X: Input training features.  Only used for unit testing. \n",
    "        :param y: Input training labels.  Only used for unit testing. \n",
    "        \"\"\"\n",
    "        \n",
    "        # initalize random seed \n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "        # initialize parameters  \n",
    "        self.n, self.M = n, margin\n",
    "        \n",
    "        # generate random simulated data \n",
    "        self.X_train, self.y_train = self.gen_data()\n",
    "        \n",
    "        # only used for unit tests \n",
    "        if X is not None and y is not None: self.X_train, self.y_train, self.n = X, y, X.shape[0]\n",
    "        \n",
    "        # initialize weights and bias to zero \n",
    "        self.w = np.array([1.0,0.0])\n",
    "        self.b = 0 \n",
    "        \n",
    "        # initialize total mistake counter \n",
    "        self.num_mistakes = 0 \n",
    "        \n",
    "    def train(self, max_epochs=1000000):\n",
    "        \"\"\"\n",
    "        Runs the Perceptron Algorithm until all training data is correctly classified. \n",
    "        \n",
    "        :param max_epochs: Maximum number of epochs to perform before stopping.  Use for development. \n",
    "        \"\"\"\n",
    "        \n",
    "        self.w = self.w \n",
    "        self.b = self.b \n",
    "            \n",
    "        \n",
    "    def plot_model(self):\n",
    "        \"\"\"\n",
    "        Plots the simulated data.  Plots the learned decision boundary (#TODO) \n",
    "        \"\"\"\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,8))\n",
    "        colors = [\"steelblue\" if yi==-1 else \"#a76c6e\" for yi in self.y_train]\n",
    "        ax.scatter(self.X_train[:,0], self.X_train[:,1], color=colors, s=75)\n",
    "        xplot=np.array([0,0])  # TODO \n",
    "        yplot=np.array([-1,1]) # TODO \n",
    "        ax.plot(xplot, yplot, color=\"black\", lw=2)\n",
    "        ax.grid(alpha=0.25)\n",
    "        ax.set_xlabel(r\"$x_1$\", fontsize=16)\n",
    "        ax.set_ylabel(r\"$x_2$\", fontsize=16)\n",
    "        \n",
    "    def gen_data(self):\n",
    "        \"\"\"\n",
    "        Generate random linearly separable data with given margin. \n",
    "        Note: You should not need to change this function \n",
    "        \"\"\"\n",
    "        flip = np.random.choice([-1,1])\n",
    "        pos_x1 = np.random.uniform(-1/np.sqrt(2), 1/np.sqrt(2), int(self.n/2))\n",
    "        pos_x2 = np.random.uniform(self.M+flip*0.1,1/np.sqrt(2),int(self.n/2))\n",
    "        pos_x2[-1] = self.M+flip*0.1 \n",
    "        neg_x1 = np.random.uniform(-1/np.sqrt(2), 1/np.sqrt(2), int(self.n/2))\n",
    "        neg_x2 = np.random.uniform(-1/np.sqrt(2),-self.M+flip*0.1,int(self.n/2))\n",
    "        neg_x2[-1] = -self.M+flip*0.1\n",
    "        X = np.concatenate((np.column_stack((pos_x1, pos_x2)), np.column_stack((neg_x1, neg_x2))))\n",
    "        X = np.dot(X, np.array([[np.cos(np.pi/6), np.sin(np.pi/6)], [-np.sin(np.pi/6), np.cos(np.pi/6)]]))\n",
    "        y = np.array([+1]*int(self.n/2) + [-1]*int(self.n/2))\n",
    "        rand_order = np.random.choice(range(self.n), replace=False, size=self.n)\n",
    "        return X[rand_order], y[rand_order]\n",
    "                                   \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: The `Perceptron` class above has the capability to generate it's own training data with certain properties. Execute the cell below to generate $n=100$ simulated training examples and plot them.  Experiment with the `margin` parameter (good values to try are between $0.01$ and $0.4$).  Explain what the `margin` parameter is doing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc = Perceptron(n=100, margin=0.2, random_state=1241)\n",
    "perc.plot_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Modify the `train` method in the `Perceptron` class to perform the Perceptron Learning Algorithm and learn weights ${\\bf w}$ and bias $b$ that perfectly classify the linearly separable training data. Your implementation should: \n",
    "\n",
    "- visit all training examples in a random shuffled order over each training epoch \n",
    "- terminate when you perform an entire epoch without making a single classification error on the training data \n",
    "- use the `self.num_mistakes` counter to count the total number of classification errors that are made over the entire training process \n",
    "\n",
    "When you think you're done, execute the cell below to run a unit test based on the example starting on Slide 9 of the the [Perceptron Lecture](https://www.cs.colorado.edu/~ketelsen/files/courses/csci4622/slides/lesson07.pdf).  \n",
    "\n",
    "**Notes**: \n",
    "- You should not use Scikit-Learn's Perceptron object in your solution. \n",
    "- It's a good idea to implement a stopping criterion based on the `max_epochs` parameter right away.  Later we'll look at training sets that will terminate on their own, but implementing this stop will save you some pain in the development process.  \n",
    "- Do not change the initial guess for the weights and bias.  These values were chosen to match the example done in lecture for the unit tests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i tests/tests.py \"prob 4A\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Modify the `plot_model` method so that it plots the learned decision boundary with the training data.  Demonstrate that your method is working by training a perceptron with a margin of your choice and displaying the resulting plot. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: In lecture we stated that the time it takes to train the Perceptron on a linearly separable training set is determined by the `margin` of the training data. Recall that the margin of a linear classifier is the distance from the decision boundary to the closest training point (Yes, I just answered **Part A** for you). \n",
    "\n",
    "An important theorem for Perceptron training states that if you train a perceptron on linearly separable training data with margin $M > 0$ and each training point satisfies $\\|{\\bf x}\\|_2 \\leq 1$ then the Perceptron Training Algorithm will complete after making at most $1/M^2$ classification mistakes (and thus, at most $1/M^2$ updates of the weights and bias). \n",
    "\n",
    "In this exercise you will empirically verify this theorem: \n",
    "- Train 20 perceptrons on randomly generated training sets of size $n=100$ with margins of $M=0.3, 0.1, 0.01, 0.001,$ and $0.0001$ (be sure to change the value of `random_state` for each run).\n",
    "- Compute the **average** number of mistakes for each margin. \n",
    "- Produce a log-log plot with $1/M$ on the horizontal axis and average number of mistakes on the vertical axis. \n",
    "- On the same set of axes, plot the theoretical upper bound on the number of training mistakes. \n",
    "\n",
    "Based on the plot, can you say that we have good empirical evidence that the theorem is true?  Briefly justify your conclusion based on your plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
